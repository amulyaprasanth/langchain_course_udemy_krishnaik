{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1a7cc4b",
   "metadata": {},
   "source": [
    "#  Simple Gen AI Application\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba98ce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129ae896",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "\n",
    "From the website we need to scrape the data\n",
    "\n",
    "`Load Data --> Docs --> Divide our text into chunks --> text --> Vector Embedding --> Vector Store DB`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38c5f817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.document_loaders.web_base.WebBaseLoader object at 0x730615b29850>\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "url = \"https://docs.langchain.com/langsmith/observability-quickstart\"\n",
    "\n",
    "loader = WebBaseLoader(url)\n",
    "print(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daa99ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='Tracing quickstart - Docs by LangChainOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KLangSmithPlatform for LLM observability and evaluationOverviewQuickstartsTrace an applicationEvaluate an applicationTest promptsAPI & SDKsAPI referencePython SDKJS/TS SDKPricingPlansPricing FAQOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KGitHubForumForumSearch...NavigationQuickstartsTracing quickstartGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pagePrerequisites1. Create a directory and install dependencies2. Set up environment variables3. Define your application4. Trace LLM calls5. Trace an entire applicationNext stepsVideo guideQuickstartsTracing quickstartCopy pageCopy pageObservability is a critical requirement for applications built with large language models (LLMs). LLMs are non-deterministic, which means that the same prompt can produce different responses. This behavior makes debugging and monitoring more challenging than with traditional software.\\nLangSmith addresses this by providing end-to-end visibility into how your application handles a request. Each request generates a trace, which captures the full record of what happened. Within a trace are individual runs, the specific operations your application performed, such as an LLM call or a retrieval step. Tracing runs allows you to inspect, debug, and validate your application’s behavior.\\nIn this quickstart, you will set up a minimal Retrieval Augmented Generation (RAG) application and add tracing with LangSmith. You will:\\n\\nConfigure your environment.\\nCreate an application that retrieves context and calls an LLM.\\nEnable tracing to capture both the retrieval step and the LLM call.\\nView the resulting traces in the LangSmith UI.\\n\\nIf you prefer to watch a video on getting started with tracing, refer to the quickstart Video guide.\\n\\u200bPrerequisites\\nBefore you begin, make sure you have:\\n\\nA LangSmith account: Sign up or log in at smith.langchain.com.\\nA LangSmith API key: Follow the Create an API key guide.\\nAn OpenAI API key: Generate this from the OpenAI dashboard.\\n\\nThe example app in this quickstart will use OpenAI as the LLM provider. You can adapt the example for your app’s LLM provider.\\nIf you’re building an application with LangChain or LangGraph, you can enable LangSmith tracing with a single environment variable. Get started by reading the guides for tracing with LangChain or tracing with LangGraph.\\n\\u200b1. Create a directory and install dependencies\\nIn your terminal, create a directory for your project and install the dependencies in your environment:\\nPythonTypeScriptCopymkdir ls-observability-quickstart && cd ls-observability-quickstart\\npython -m venv .venv && source .venv/bin/activate\\npython -m pip install --upgrade pip\\npip install -U langsmith openai\\n\\n\\u200b2. Set up environment variables\\nSet the following environment variables:\\n\\nLANGSMITH_TRACING\\nLANGSMITH_API_KEY\\nOPENAI_API_KEY (or your LLM provider’s API key)\\n(optional) LANGSMITH_WORKSPACE_ID: If your LangSmith API is linked to multiple workspaces, set this variable to specify which workspace to use.\\n\\nCopyexport LANGSMITH_TRACING=true\\nexport LANGSMITH_API_KEY=\"<your-langsmith-api-key>\"\\nexport OPENAI_API_KEY=\"<your-openai-api-key>\"\\nexport LANGSMITH_WORKSPACE_ID=\"<your-workspace-id>\"\\n\\nIf you’re using Anthropic, use the Anthropic wrapper to trace your calls. For other providers, use the traceable wrapper.\\n\\u200b3. Define your application\\nYou can use the example app code outlined in this step to instrument a RAG application. Or, you can use your own application code that includes an LLM call.\\nThis is a minimal RAG app that uses the OpenAI SDK directly without any LangSmith tracing added yet. It has three main parts:\\n\\nRetriever function: Simulates document retrieval that always returns the same string.\\nOpenAI client: Instantiates a plain OpenAI client to send a chat completion request.\\nRAG function: Combines the retrieved documents with the user’s question to form a system prompt, calls the chat.completions.create() endpoint with gpt-4o-mini, and returns the assistant’s response.\\n\\nAdd the following code into your app file (e.g., app.py or app.ts):\\nPythonTypeScriptCopyfrom openai import OpenAI\\n\\ndef retriever(query: str):\\n    # Minimal example retriever\\n    return [\"Harrison worked at Kensho\"]\\n\\n# OpenAI client call (no wrapping yet)\\nclient = OpenAI()\\n\\ndef rag(question: str) -> str:\\n    docs = retriever(question)\\n    system_message = (\\n        \"Answer the user\\'s question using only the provided information below:\\\\n\"\\n        + \"\\\\n\".join(docs)\\n    )\\n\\n    # This call is not traced yet\\n    resp = client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        messages=[\\n            {\"role\": \"system\", \"content\": system_message},\\n            {\"role\": \"user\", \"content\": question},\\n        ],\\n    )\\n    return resp.choices[0].message.content\\n\\nif __name__ == \"__main__\":\\n    print(rag(\"Where did Harrison work?\"))\\n\\n\\u200b4. Trace LLM calls\\nTo start, you’ll trace all your OpenAI calls. LangSmith provides wrappers:\\n\\nPython: wrap_openai\\nTypeScript: wrapOpenAI\\n\\nThis snippet wraps the OpenAI client so that every subsequent model call is logged automatically as a traced child run in LangSmith.\\n\\n\\nInclude the highlighted lines in your app file:\\nPythonTypeScriptCopyfrom openai import OpenAI\\nfrom langsmith.wrappers import wrap_openai  # traces openai calls\\n\\ndef retriever(query: str):\\n    return [\"Harrison worked at Kensho\"]\\n\\nclient = wrap_openai(OpenAI())  # log traces by wrapping the model calls\\n\\ndef rag(question: str) -> str:\\n    docs = retriever(question)\\n    system_message = (\\n        \"Answer the user\\'s question using only the provided information below:\\\\n\"\\n        + \"\\\\n\".join(docs)\\n    )\\n    resp = client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        messages=[\\n            {\"role\": \"system\", \"content\": system_message},\\n            {\"role\": \"user\", \"content\": question},\\n        ],\\n    )\\n    return resp.choices[0].message.content\\n\\nif __name__ == \"__main__\":\\n    print(rag(\"Where did Harrison work?\"))\\n\\n\\n\\nCall your application:\\nPythonTypeScriptCopypython app.py\\n\\nYou’ll receive the following output:\\nCopyHarrison worked at Kensho.\\n\\n\\n\\nIn the LangSmith UI, navigate to the default Tracing Project for your workspace (or the workspace you specified in Step 2). You’ll see the OpenAI call you just instrumented.\\n\\n\\n\\n\\u200b5. Trace an entire application\\nYou can also use the traceable decorator for Python or TypeScript to trace your entire application instead of just the LLM calls.\\n\\n\\nInclude the highlighted code in your app file:\\nPythonTypeScriptCopyfrom openai import OpenAI\\nfrom langsmith.wrappers import wrap_openai\\nfrom langsmith import traceable\\n\\ndef retriever(query: str):\\n    return [\"Harrison worked at Kensho\"]\\n\\nclient = wrap_openai(OpenAI())  # keep this to capture the prompt and response from the LLM\\n\\n@traceable\\ndef rag(question: str) -> str:\\n    docs = retriever(question)\\n    system_message = (\\n        \"Answer the user\\'s question using only the provided information below:\\\\n\"\\n        + \"\\\\n\".join(docs)\\n    )\\n    resp = client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        messages=[\\n            {\"role\": \"system\", \"content\": system_message},\\n            {\"role\": \"user\", \"content\": question},\\n        ],\\n    )\\n    return resp.choices[0].message.content\\n\\nif __name__ == \"__main__\":\\n    print(rag(\"Where did Harrison work?\"))\\n\\n\\n\\nCall the application again to create a run:\\nPythonTypeScriptCopypython app.py\\n\\n\\n\\nReturn to the LangSmith UI, navigate to the default Tracing Project for your workspace (or the workspace you specified in Step 2). You’ll find a trace of the entire app pipeline with the rag step and the ChatOpenAI LLM call.\\n\\n\\n\\n\\u200bNext steps\\nHere are some topics you might want to explore next:\\n\\nTracing integrations provide support for various LLM providers and agent frameworks.\\nFiltering traces can help you effectively navigate and analyze data in tracing projects that contain a significant amount of data.\\nTrace a RAG application is a full tutorial, which adds observability to an application from development through to production.\\nSending traces to a specific project changes the destination project of your traces.\\n\\n\\u200bVideo guide\\nWas this page helpful?YesNoOverviewEvaluate an applicationAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c0b6d5",
   "metadata": {},
   "source": [
    "## Divide our text into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4407f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81a54d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='Tracing quickstart - Docs by LangChainOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KLangSmithPlatform for LLM observability and evaluationOverviewQuickstartsTrace an applicationEvaluate an applicationTest promptsAPI & SDKsAPI referencePython SDKJS/TS SDKPricingPlansPricing FAQOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KGitHubForumForumSearch...NavigationQuickstartsTracing quickstartGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pagePrerequisites1. Create a directory and install dependencies2. Set up environment variables3. Define your application4. Trace LLM calls5. Trace an entire applicationNext stepsVideo guideQuickstartsTracing quickstartCopy pageCopy pageObservability is a critical requirement for'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='variables3. Define your application4. Trace LLM calls5. Trace an entire applicationNext stepsVideo guideQuickstartsTracing quickstartCopy pageCopy pageObservability is a critical requirement for applications built with large language models (LLMs). LLMs are non-deterministic, which means that the same prompt can produce different responses. This behavior makes debugging and monitoring more challenging than with traditional software.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='LangSmith addresses this by providing end-to-end visibility into how your application handles a request. Each request generates a trace, which captures the full record of what happened. Within a trace are individual runs, the specific operations your application performed, such as an LLM call or a retrieval step. Tracing runs allows you to inspect, debug, and validate your application’s behavior.\\nIn this quickstart, you will set up a minimal Retrieval Augmented Generation (RAG) application and add tracing with LangSmith. You will:'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='Configure your environment.\\nCreate an application that retrieves context and calls an LLM.\\nEnable tracing to capture both the retrieval step and the LLM call.\\nView the resulting traces in the LangSmith UI.\\n\\nIf you prefer to watch a video on getting started with tracing, refer to the quickstart Video guide.\\n\\u200bPrerequisites\\nBefore you begin, make sure you have:\\n\\nA LangSmith account: Sign up or log in at smith.langchain.com.\\nA LangSmith API key: Follow the Create an API key guide.\\nAn OpenAI API key: Generate this from the OpenAI dashboard.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='A LangSmith account: Sign up or log in at smith.langchain.com.\\nA LangSmith API key: Follow the Create an API key guide.\\nAn OpenAI API key: Generate this from the OpenAI dashboard.\\n\\nThe example app in this quickstart will use OpenAI as the LLM provider. You can adapt the example for your app’s LLM provider.\\nIf you’re building an application with LangChain or LangGraph, you can enable LangSmith tracing with a single environment variable. Get started by reading the guides for tracing with LangChain or tracing with LangGraph.\\n\\u200b1. Create a directory and install dependencies\\nIn your terminal, create a directory for your project and install the dependencies in your environment:\\nPythonTypeScriptCopymkdir ls-observability-quickstart && cd ls-observability-quickstart\\npython -m venv .venv && source .venv/bin/activate\\npython -m pip install --upgrade pip\\npip install -U langsmith openai\\n\\n\\u200b2. Set up environment variables\\nSet the following environment variables:')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aef6c4",
   "metadata": {},
   "source": [
    "## Convert into Vectors using Embeddings and storing it into vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e76fef9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prasanth/projects/Langchain/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x7304de043710>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x7304dd743c50>, model='text-embedding-ada-002', dimensions=None, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1377b6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstore_db = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45335921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x7304dc4abf20>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0137ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangSmith addresses this by providing end-to-end visibility into how your application handles a request. Each request generates a trace, which captures the full record of what happened. Within a trace are individual runs, the specific operations your application performed, such as an LLM call or a retrieval step. Tracing runs allows you to inspect, debug, and validate your application’s behavior.\\nIn this quickstart, you will set up a minimal Retrieval Augmented Generation (RAG) application and add tracing with LangSmith. You will:'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query from vector store db\n",
    "query = \" Each request generates a trace, which captures the full record of what happened. \"\n",
    "result = vectorstore_db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31d5ff4",
   "metadata": {},
   "source": [
    "## Retrieval Chain, Documents Chain\n",
    "\n",
    "`Chain` - A chain is just a sequence of steps where inputs/outputs flow between an LLM, prompt templates, retrievers, etc.\n",
    "`Retrieval Chain` - Interfact between input and vectordb and pass it to the llm\n",
    "\n",
    "`Document Chain` - a document chain is a special type of chain designed to handle multiple documents at once and combine them into a single response.\n",
    "\n",
    "`stuff_documents_chain` - It takes a list of documents (retrieved from a vector store or elsewhere).It \"stuffs\" all of their contents together into a single prompt string.That combined string is then passed to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "919de83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698536e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=' Answer the following question based only on the provided context:\\n    <context>\\n    {context}\\n    </context>\\n     '), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7304d2f765a0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7304d2fcaab0>, root_client=<openai.OpenAI object at 0x7304dc590980>, root_async_client=<openai.AsyncOpenAI object at 0x7304d2fcae10>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\" Answer the following question based only on the provided context:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "     \"\"\"\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7ed6509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What steps will I take in the quickstart for setting up the RAG application with LangSmith tracing?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "document_chain.invoke({\n",
    "    \"input\": \"you will set up a minimal Retrieval Augmented Generation\",\n",
    "    \"context\": [Document(page_content='LangSmith addresses this by providing end-to-end visibility into how your application handles a request. Each request generates a trace, which captures the full record of what happened. Within a trace are individual runs, the specific operations your application performed, such as an LLM call or a retrieval step. Tracing runs allows you to inspect, debug, and validate your application’s behavior.\\nIn this quickstart, you will set up a minimal Retrieval Augmented Generation (RAG) application and add tracing with LangSmith. You will:'\n",
    "    )]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ba09c9",
   "metadata": {},
   "source": [
    "However, we want the documents to first come from the retriever we just set up. That way, we can use the retriever to dynamically select the most relevant documents and pass those in for a given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6c9659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "\n",
    "retriever= vectorstore_db.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e24005",
   "metadata": {},
   "source": [
    "In LangChain, we combine a retriever with a document chain to form a retrieval chain because each serves a different purpose in the RAG pipeline. The retriever’s job is to search a knowledge base (like a vector store) and return the most relevant documents for a given query, while the document chain’s job is to take those documents along with the user’s question, format them into a prompt, and pass them to the LLM to generate a final answer. By combining them, we get an end-to-end system where the retriever finds the right context and the document chain ensures the LLM uses that context effectively, enabling accurate and context-aware responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2f8d150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was the page described in the context helpful?\n"
     ]
    }
   ],
   "source": [
    "## Get the response from the llm\n",
    "response = retrieval_chain.invoke({\"input\": \"Summarize the entire page\"})\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "031e0b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Summarize the entire page',\n",
       " 'context': [Document(id='e94566f0-4da7-4562-a304-60d9be4b2faa', metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='\\u200bVideo guide\\nWas this page helpful?YesNoOverviewEvaluate an applicationAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       "  Document(id='8b0117b4-0d35-4c0a-ba2d-71bb28712a2d', metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='variables3. Define your application4. Trace LLM calls5. Trace an entire applicationNext stepsVideo guideQuickstartsTracing quickstartCopy pageCopy pageObservability is a critical requirement for applications built with large language models (LLMs). LLMs are non-deterministic, which means that the same prompt can produce different responses. This behavior makes debugging and monitoring more challenging than with traditional software.'),\n",
       "  Document(id='39b4960f-65bf-48e2-bfc5-ebdc51194aa9', metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='LangSmith addresses this by providing end-to-end visibility into how your application handles a request. Each request generates a trace, which captures the full record of what happened. Within a trace are individual runs, the specific operations your application performed, such as an LLM call or a retrieval step. Tracing runs allows you to inspect, debug, and validate your application’s behavior.\\nIn this quickstart, you will set up a minimal Retrieval Augmented Generation (RAG) application and add tracing with LangSmith. You will:'),\n",
       "  Document(id='3a68dbde-7c36-41c5-8afa-cdbe4ad841a5', metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='Tracing quickstart - Docs by LangChainOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KLangSmithPlatform for LLM observability and evaluationOverviewQuickstartsTrace an applicationEvaluate an applicationTest promptsAPI & SDKsAPI referencePython SDKJS/TS SDKPricingPlansPricing FAQOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KGitHubForumForumSearch...NavigationQuickstartsTracing quickstartGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pagePrerequisites1. Create a directory and install dependencies2. Set up environment variables3. Define your application4. Trace LLM calls5. Trace an entire applicationNext stepsVideo guideQuickstartsTracing quickstartCopy pageCopy pageObservability is a critical requirement for')],\n",
       " 'answer': 'Was the page described in the context helpful?'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d78d1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
